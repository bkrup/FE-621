---
title: "Homework 4"
author: "Brendon Krupa"
date: "5/2/2020"
output: word_document
---

# Problem 4: Parameter Estimation

```{r}
setwd("/Users/Brendon/Documents/FE 621/HW 4")
df <- read.csv("2020SpringHW4SampleData.csv")
dt <- 0.0001
df <- ts(df[,2:4],deltat = dt)
head(df)
```

## Model 1:

dSt = theta1 * St * dt + theta2 * St^theta3 * dWt

```{r}
library(Sim.DiffProc)
f1 <- expression(theta[1]*x) # drift
g1 <- expression(theta[2]*x^theta[3]) # diffusion 
aics1 <- c()
bics1 <- c()
logLs1 <- c()
for (i in 1:3) {
  fit <- fitsde(data = df[,i], drift = f1, diffusion = g1, start = list(theta1=1,theta2=1,theta3=1))
  aics1 <- c(aics1,AIC(fit))
  bics1 <- c(bics1,BIC(fit))
  logLs1 <- c(logLs1,logLik(fit))
}
aic_sum_table <- data.frame(mydata1=aics1[1],mydata2=aics1[2],mydata3=aics1[3],row.names = c("Model 1"))
bic_sum_table <- data.frame(mydata1=bics1[1],mydata2=bics1[2],mydata3=bics1[3],row.names = c("Model 1"))
logL_sum_table <- data.frame(mydata1=logLs1[1],mydata2=logLs1[2],mydata3=logLs1[3],row.names = c("Model 1"))
```

## Model 2:

dSt = (theta1 + theta2 * St)*dt + theta3 * St^theta4 * dWt

```{r}
f2 <- expression(theta[1] + theta[2]*x)
g2 <- expression(theta[3]*x^theta[4])
aics2 <- c()
bics2 <- c()
logLs2 <- c()
for (i in 1:3) {
  fit <- fitsde(data = df[,1], drift = f2, diffusion = g2, start = list(theta1=1,theta2=1,theta3=1,theta4=1))
  aics2 <- c(aics2,AIC(fit))
  bics2 <- c(bics2,BIC(fit))
  logLs2 <- c(logLs2,logLik(fit))
}
aic_sum_table <- rbind(aic_sum_table,data.frame(mydata1=aics2[1],mydata2=aics2[2],mydata3=aics2[3],row.names = c("Model 2")))
bic_sum_table <- rbind(bic_sum_table,data.frame(mydata1=bics2[1],mydata2=bics2[2],mydata3=bics2[3],row.names = c("Model 2")))
logL_sum_table <- rbind(logL_sum_table,data.frame(mydata1=logLs2[1],mydata2=logLs2[2],mydata3=logLs2[3],row.names = c("Model 2")))
```

## Model 3:

dSt = (theta1 + theta2 * St)*dt + theta3*sqrt(St)*dWt

```{r}
f3 <- expression(theta[1] + theta[2]*x)
g3 <- expression(theta[3]*sqrt(x))
aics3 <- c()
bics3 <- c()
logLs3 <- c()
for (i in 1:3) {
  fit <- fitsde(data = df[,i], drift = f3, diffusion = g3, start = list(theta1=1,theta2=1,theta3=1))
  aics3 <- c(aics3,AIC(fit))
  bics3 <- c(bics3,BIC(fit))
  logLs3 <- c(logLs3,logLik(fit))
}
aic_sum_table <- rbind(aic_sum_table,data.frame(mydata1=aics3[1],mydata2=aics3[2],mydata3=aics3[3],row.names = c("Model 3")))
bic_sum_table <- rbind(bic_sum_table,data.frame(mydata1=bics3[1],mydata2=bics3[2],mydata3=bics3[3],row.names = c("Model 3")))
logL_sum_table <- rbind(logL_sum_table,data.frame(mydata1=logLs3[1],mydata2=logLs3[2],mydata3=logLs3[3],row.names = c("Model 3")))
```


## Model 4:

dSt = theta1 * dt + theta2 * St^theta3*dWt

```{r}
f4 <- expression(theta[1])
g4 <- expression(theta[2]*x^theta[3])
aics4 <- c()
bics4 <- c()
logLs4 <- c()
for (i in 1:3) {
  fit <- fitsde(data = df[,i], drift = f4, diffusion = g4, start = list(theta1=1,theta2=1,theta3=1))
  aics4 <- c(aics4,AIC(fit))
  bics4 <- c(bics4,BIC(fit))
  logLs4 <- c(logLs4,logLik(fit))
}
aic_sum_table <- rbind(aic_sum_table,data.frame(mydata1=aics4[1],mydata2=aics4[2],mydata3=aics4[3],row.names = c("Model 4")))
bic_sum_table <- rbind(bic_sum_table,data.frame(mydata1=bics4[1],mydata2=bics4[2],mydata3=bics4[3],row.names = c("Model 4")))
logL_sum_table <- rbind(logL_sum_table,data.frame(mydata1=logLs4[1],mydata2=logLs4[2],mydata3=logLs4[3],row.names = c("Model 4")))
```


## Model 5

dSt = theta1 * St*dt + (theta2 + theta3 * St^theta4)*dWt

```{r}
f5 <- expression(theta[1]*x)
g5 <- expression(theta[2] + theta[3]*x^theta[4])
aics5 <- c()
bics5 <- c()
logLs5 <- c()
for (i in 1:3) {
  fit <- fitsde(data = df[,i], drift = f5, diffusion = g5, start = list(theta1=1,theta2=1,theta3=1,theta4=1))
  aics5 <- c(aics5,AIC(fit))
  bics5 <- c(bics5,BIC(fit))
  logLs5 <- c(logLs5,logLik(fit))
}
aic_sum_table <- rbind(aic_sum_table,data.frame(mydata1=aics5[1],mydata2=aics5[2],mydata3=aics5[3],row.names = c("Model 5")))
bic_sum_table <- rbind(bic_sum_table,data.frame(mydata1=bics5[1],mydata2=bics5[2],mydata3=bics5[3],row.names = c("Model 5")))
logL_sum_table <- rbind(logL_sum_table,data.frame(mydata1=logLs5[1],mydata2=logLs5[2],mydata3=logLs5[3],row.names = c("Model 5")))
```

## Results Tables

```{r}
print(paste("AIC Table"))
aic_sum_table
print(paste("BIC Table"))
bic_sum_table
print(paste("Extract Log-Likelihood Table"))
logL_sum_table
```

After applying the Euler method to each model and each data set the results show the first column of data, mydata1, most closely resembles model 1 since the model yields the smallest AIC and BIC while also having the largest Log-Likelihood. Model 4 can be assumed to be the source of the second column for the same reasons and model 2 is is signifcantly the best fit for the third column with respect to all three metrics.


## Comparing Parameters From Each Method

### Fitting Model 1 to Mydata1

```{r}
pmle <- c("euler","ozaki","shoji","kessler")
fits1_1 <- lapply(1:4, function(i) fitsde(df[,1],drift=f1,diffusion=g1,pmle=pmle[i],start = list(theta1=1,theta2=1,theta3=1)))
coef_table1 <- rbind(coef(fits1_1[[1]]),coef(fits1_1[[2]]),coef(fits1_1[[3]]),coef(fits1_1[[4]]))
rownames(coef_table1) <- pmle

perf_table1 <- rbind(cbind(AIC(fits1_1[[1]]),BIC(fits1_1[[1]]),logLik(fits1_1[[1]])),
                     cbind(AIC(fits1_1[[2]]),BIC(fits1_1[[2]]),logLik(fits1_1[[2]])),
                     cbind(AIC(fits1_1[[3]]),BIC(fits1_1[[3]]),logLik(fits1_1[[3]])),
                     cbind(AIC(fits1_1[[4]]),BIC(fits1_1[[4]]),logLik(fits1_1[[4]])))
rownames(perf_table1) <- pmle
colnames(perf_table1) <- c("AIC", "BIC", "Log-Like")

se_table1 <- cbind((confint(fits1_1[[1]])[,2]-confint(fits1_1[[1]])[,1])/(2*1.96),
                   (confint(fits1_1[[2]])[,2]-confint(fits1_1[[2]])[,1])/(2*1.96),
                   (confint(fits1_1[[3]])[,2]-confint(fits1_1[[3]])[,1])/(2*1.96),
                   (confint(fits1_1[[4]])[,2]-confint(fits1_1[[4]])[,1])/(2*1.96))
rownames(se_table1) <- c("theta1", "theta2", "theta3")
colnames(se_table1) <- pmle

print(paste("Table of coefficients from each method"))
coef_table1
print(paste("Table of standard error for each coefficient from each method"))
se_table1
print(paste("Table of performance metrics from each method"))
perf_table1
```


## Fitting Model 4 to Mydata2

```{r}
fits4_2 <- lapply(1:4, function(i) fitsde(df[,2],drift=f4,diffusion=g4,pmle=pmle[i],start = list(theta1=1,theta2=1,theta3=1)))
coef_table2 <- rbind(coef(fits4_2[[1]]),coef(fits4_2[[2]]),coef(fits4_2[[3]]),coef(fits4_2[[4]]))
rownames(coef_table2) <- pmle

perf_table2 <- rbind(cbind(AIC(fits4_2[[1]]),BIC(fits4_2[[1]]),logLik(fits4_2[[1]])),
                     cbind(AIC(fits4_2[[2]]),BIC(fits4_2[[2]]),logLik(fits4_2[[2]])),
                     cbind(AIC(fits4_2[[3]]),BIC(fits4_2[[3]]),logLik(fits4_2[[3]])),
                     cbind(AIC(fits4_2[[4]]),BIC(fits4_2[[4]]),logLik(fits4_2[[4]])))
rownames(perf_table2) <- pmle
colnames(perf_table2) <- c("AIC", "BIC", "Log-Like")

se_table2 <- cbind((confint(fits4_2[[1]])[,2]-confint(fits4_2[[1]])[,1])/(2*1.96),
                   (confint(fits4_2[[4]])[,2]-confint(fits4_2[[4]])[,1])/(2*1.96))
rownames(se_table2) <- c("theta1", "theta2", "theta3")
colnames(se_table2) <- c(pmle[1],pmle[4])

print(paste("Table of coefficients from each method"))
coef_table2
print(paste("Table of standard error for each coefficient from each method"))
se_table2
print(paste("Table of performance metrics from each method"))
perf_table2
```


## Fitting Model 2 to Mydata3

```{r}
fits2_3 <- lapply(1:4, function(i) fitsde(df[,3],drift=f2,diffusion=g2,pmle=pmle[i],start = list(theta1=1,theta2=1,theta3=1,theta4=1)))
coef_table3 <- rbind(coef(fits2_3[[1]]),coef(fits2_3[[2]]),coef(fits2_3[[3]]),coef(fits2_3[[4]]))
rownames(coef_table3) <- pmle

perf_table3 <- rbind(cbind(AIC(fits2_3[[1]]),BIC(fits2_3[[1]]),logLik(fits2_3[[1]])),
                     cbind(AIC(fits2_3[[2]]),BIC(fits2_3[[2]]),logLik(fits2_3[[2]])),
                     cbind(AIC(fits2_3[[3]]),BIC(fits2_3[[3]]),logLik(fits2_3[[3]])),
                     cbind(AIC(fits2_3[[4]]),BIC(fits2_3[[4]]),logLik(fits2_3[[4]])))
rownames(perf_table3) <- pmle
colnames(perf_table3) <- c("AIC", "BIC", "Log-Like")

se_table3 <- cbind((confint(fits2_3[[1]])[,2]-confint(fits2_3[[1]])[,1])/(2*1.96),
                   (confint(fits2_3[[2]])[,2]-confint(fits2_3[[2]])[,1])/(2*1.96),
                   (confint(fits2_3[[3]])[,2]-confint(fits2_3[[3]])[,1])/(2*1.96),
                   (confint(fits2_3[[4]])[,2]-confint(fits2_3[[4]])[,1])/(2*1.96))
rownames(se_table3) <- c("theta1", "theta2", "theta3", "theta4")
colnames(se_table3) <- pmle

print(paste("Table of coefficients from each method"))
coef_table3
print(paste("Table of standard error for each coefficient from each method"))
se_table3
print(paste("Table of performance metrics from each method"))
perf_table3
```

This leads me to the conclusion that the Euler method yields the best estimates. The tables show the coefficients and standard errors from the Euler and Kessler methods are very close in all three cases. The Ozaki and Shoji-Ozaki methods were unable to output meaningful results for the second data set suggesting that they are not as versatile as the others, and seeing that they do not provide a signifcantly better fit in the other sets, I am reluctant to choose either method as the one to provide the best estimates. The Shoji-Ozaki method did find slightly lower information criteria and standard error for the last data set with signifcantly different estimates for theta1 and theta2 from the other methods; however, the difference in standard error is not signifcant enough to warant selection over the Euler method. The Kessler method employs a more complicated high order Taylor expansion approach to parameterization; however, this fails to yield a signifcant benefit over the Euler method in terms of information criteria or standard error and actually underperforms in some cases. Since the added complexity of this method adds significantly to the run time without any significant benefit in performance, I have come to the conclusion that the Euler method provides the best estimates since its simplicity relative to the alternatives allows it to run faster and be more flexible, and these benefits do not come at the expense of accuracy.
